{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Text Mining and Search\n",
            "\n",
            "UniMiB 2022/23"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "**IMDB Reviews**"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# initial imports\n",
            "\n",
            "import io\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "from tqdm import tqdm\n",
            "import os\n",
            "import joblib"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Merge data - Train"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_dataset = pd.DataFrame(columns=['text','sentiment'])\n",
            "\n",
            "found = 0\n",
            "for file in tqdm(os.listdir('../data/raw/train/neg/')):\n",
            "  with io.open('../data/raw/train/neg/'+file, mode=\"r\", encoding=\"utf-8\") as f:\n",
            "    text = f.read()\n",
            "    train_dataset.loc[len(train_dataset)] = [text, 'NEG']\n",
            "for file in tqdm(os.listdir('../data/raw/train/pos/')):\n",
            "  with io.open('../data/raw/train/pos/'+file, mode=\"r\", encoding=\"utf-8\") as f:\n",
            "    text = f.read()\n",
            "    train_dataset.loc[len(train_dataset)] = [text, 'POS']\n",
            "            "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_dataset.to_csv('../data/train_dataset.csv')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Merge data - Test"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_dataset = pd.DataFrame(columns=['text', 'sentiment'])\n",
            "\n",
            "found = 0\n",
            "for file in tqdm(os.listdir('../data/raw/test/neg/')):\n",
            "  with io.open('../data/raw/test/neg/'+file, mode=\"r\", encoding=\"utf-8\") as f:\n",
            "    text = f.read()\n",
            "    test_dataset.loc[len(test_dataset)] = [text, 'NEG']\n",
            "for file in tqdm(os.listdir('../data/raw/test/pos/')):\n",
            "  with io.open('../data/raw/test/pos/'+file, mode=\"r\", encoding=\"utf-8\") as f:\n",
            "    text = f.read()\n",
            "    test_dataset.loc[len(test_dataset)] = [text, 'POS']\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_dataset.to_csv('../data/test_dataset.csv')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Read data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train = pd.read_csv('../data/train_dataset.csv')\n",
            "train = train[['text', 'sentiment']]\n",
            "train.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test = pd.read_csv('../data/test_dataset.csv')\n",
            "test = test[['text', 'sentiment']]\n",
            "test.head()"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Text Pre-Processing"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import string\n",
            "import re\n",
            "\n",
            "import nltk\n",
            "from nltk.stem import WordNetLemmatizer\n",
            "from nltk.corpus import stopwords\n",
            "from nltk.tokenize import word_tokenize\n",
            "nltk.download('stopwords')\n",
            "\n",
            "from preprocess import *"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "lemmatizer = WordNetLemmatizer()\n",
            "\n",
            "# function to remove all the numbers from the text\n",
            "def remove_numbers(text_to_preprocess):\n",
            "    return re.sub(r'\\d+', '', text_to_preprocess)\n",
            "\n",
            "# function to remove all the punctuation marks from the text\n",
            "def remove_punctuation(text):\n",
            "    return text[0].translate(str.maketrans('', '', string.punctuation))\n",
            "\n",
            "# function to remove all the stopwords from the text\n",
            "def remove_stopwords(text):\n",
            "    no_stopwords = ''\n",
            "    for item in text.split():\n",
            "      if item not in stopwords.words():\n",
            "        no_stopwords+=' '+item\n",
            "    return no_stopwords\n",
            "\n",
            "# function to find the Part-Of-Speech tags for the words in the text\n",
            "def postagger(token_words):\n",
            "    return nltk.pos_tag(token_words)\n",
            "\n",
            "# function to remove extra whitespaces from the text\n",
            "def remove_extra_whitespace(text):\n",
            "    return \" \".join(text.split())\n",
            "\n",
            "# function to tokenize the text into words\n",
            "def tokenizer(text):\n",
            "  return word_tokenize(text)\n",
            "\n",
            "# function to lemmatize the tokenized words\n",
            "def lemmatizer_function(tokenized_text):\n",
            "  lemmatized_text= ''\n",
            "  for token in tokenized_text:\n",
            "    lemmatized = lemmatizer.lemmatize(token)\n",
            "    lemmatized_text += ' '+lemmatized\n",
            "  return lemmatized_text\n",
            "\n",
            "\n",
            "# function to preprocess the text by lowercasing, removing numbers, punctuation, stopwords, extra whitespaces and lemmatizing\n",
            "def preprocess_text(text):\n",
            "    text = text.str.lower()\n",
            "    no_nums = remove_numbers(text),\n",
            "    no_punct = remove_punctuation(no_nums)\n",
            "    no_stopw = remove_stopwords(no_punct)\n",
            "    no_whtspace = remove_extra_whitespace(no_stopw)\n",
            "    tokenized = tokenizer(no_whtspace)\n",
            "    lemmatized = lemmatizer_function(tokenized)\n",
            "    return lemmatized"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "from multiprocessing import Pool\n",
            "from preprocess import preprocess_loader\n",
            "if __name__ == '__main__':\n",
            "  df_split = np.array_split(train_ds, 10)\n",
            "  pool = Pool(10)\n",
            "  df = pd.concat(pool.map(preprocess_loader, df_split))\n",
            "  pool.close()\n",
            "  pool.join()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "tqdm.pandas()\n",
            "\n",
            "train['preprocessed_text'] = train['text'].progress_apply(preprocess_text)\n",
            "train.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from bs4 import BeautifulSoup\n",
            "\n",
            "# removing the html strips\n",
            "def strip_html(text):\n",
            "    soup = BeautifulSoup(text, \"html.parser\")\n",
            "    return soup.get_text()\n",
            "\n",
            "# removing the square brackets\n",
            "def remove_between_square_brackets(text):\n",
            "    return re.sub('\\[[^]]*\\]', '', text)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Data Exploration"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_pp = pd.read_csv('../data/preprocessed_train.csv')\n",
            "train_pp = train_pp[['text', 'sentiment', 'preprocessed_text']]\n",
            "train_pp.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "test_pp = pd.read_csv('../data/preprocessed_test.csv')\n",
            "test_pp = test_pp[['text', 'sentiment', 'preprocessed_text']]\n",
            "test_pp.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from wordcloud import WordCloud, STOPWORDS\n",
            "import matplotlib.pyplot as plt\n",
            "\n",
            "# get the text and join all the reviews in training set\n",
            "# creating the text variable\n",
            "text = \" \".join(cat for cat in train_pp.preprocessed_text)\n",
            "\n",
            "# limit the word count and set the stopwords\n",
            "wordcount = 500\n",
            "stopwords = set(STOPWORDS)\n",
            "stopwords.add(\"english\")\n",
            "\n",
            "# setup, generate and save the word cloud image to a file\n",
            "wc = WordCloud(scale=5, \n",
            "               background_color=\"grey\", \n",
            "               max_words=wordcount, \n",
            "               stopwords=stopwords)\n",
            "wc.generate(text)\n",
            "wc.to_file(\"../figures/WordCloud_train.png\")\n",
            "\n",
            "# show the wordcloud as output\n",
            "plt.imshow(wc, interpolation='bilinear')\n",
            "plt.axis(\"off\")\n",
            "plt.figure()\n",
            "plt.axis(\"off\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# get the text and join all the reviews in test set\n",
            "# creating the text variable\n",
            "text = \" \".join(cat for cat in test_pp.preprocessed_text)\n",
            "\n",
            "# limit the word count and set the stopwords\n",
            "wordcount = 500\n",
            "stopwords = set(STOPWORDS)\n",
            "stopwords.add(\"english\")\n",
            "\n",
            "# setup, generate and save the word cloud image to a file\n",
            "wc = WordCloud(scale=5, \n",
            "               background_color=\"grey\", \n",
            "               max_words=wordcount, \n",
            "               stopwords=stopwords)\n",
            "wc.generate(text)\n",
            "wc.to_file(\"../figures/WordCloud_test.png\")\n",
            "\n",
            "# show the wordcloud as output\n",
            "plt.imshow(wc, interpolation='bilinear')\n",
            "plt.axis(\"off\")\n",
            "plt.figure()\n",
            "plt.axis(\"off\")\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# print  number of unique words\n",
            "print(\"Number of words: \")\n",
            "print(len(np.unique(np.hstack(train_pp.preprocessed_text))))\n",
            "print ()\n",
            "\n",
            "# print the average review length\n",
            "print(\"Average review length:\")\n",
            "result = [len(x) for x in train_pp.preprocessed_text]\n",
            "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# print  number of unique words\n",
            "print(\"Number of words: \")\n",
            "print(len(np.unique(np.hstack(test_pp.preprocessed_text))))\n",
            "print ()\n",
            "\n",
            "# print the average review length\n",
            "print(\"Average review length:\")\n",
            "result = [len(x) for x in test_pp.preprocessed_text]\n",
            "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Text Representation"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.feature_extraction.text import CountVectorizer      #-- Bag of Words\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer      #-- Tf-Idf\n",
            "\n",
            "import joblib"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "text_preprocessed_train = train_pp['preprocessed_text']\n",
            "text_preprocessed_test = test_pp['preprocessed_text']"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Bag-of-Word (BoW)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "vectorizer = CountVectorizer(ngram_range=(1,2),                                 #-- Uni-grams and Bi-grams\n",
            "                             max_features = 25000)                              #-- Most 25000 frequent grams across the text\n",
            "\n",
            "X_text_bow =  vectorizer.fit_transform(text_preprocessed_train).toarray()\n",
            "y_text_bow =  vectorizer.transform(text_preprocessed_test).toarray()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(X_text_bow.shape)\n",
            "print(y_text_bow.shape)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(X_text_bow, 'processed_train_bow.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(y_text_bow, 'processed_test_bow.save')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Binary"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "'''\n",
            "text_preprocessed = train_pp['preprocessed_text']\n",
            "\n",
            "vectorizer = CountVectorizer(binary = True, max_features = 25000)\n",
            "X_text_binary =  vectorizer.fit_transform(text_preprocessed)\n",
            "'''"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "'''\n",
            "print(X_text_bow.shape)\n",
            "'''"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "'''\n",
            "import joblib\n",
            "joblib.dump(X_text_bow, 'processed_train_binary_bow.save')\n",
            "'''"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "TF-IDF"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "vectorizer = TfidfVectorizer(max_features=25000, ngram_range=(1, 2))\n",
            "\n",
            "X_text_tfidf =  vectorizer.fit_transform(text_preprocessed_train).toarray()\n",
            "y_text_tfidf =  vectorizer.transform(text_preprocessed_test).toarray()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(X_text_tfidf.shape)\n",
            "print(y_text_tfidf.shape)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(X_text_tfidf, 'processed_train_tfidf.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(y_text_tfidf, 'processed_test_tfidf.save')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Text Classification"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
            "import joblib\n",
            "from sklearn.preprocessing import LabelEncoder"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "labels_train = train_pp['sentiment']\n",
            "\n",
            "encoder = LabelEncoder()\n",
            "encoder.fit(labels_train)\n",
            "encoded_labels_train = encoder.transform(labels_train)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "labels_test = test_pp['sentiment']\n",
            "\n",
            "encoder = LabelEncoder()\n",
            "encoder.fit(labels_test)\n",
            "encoded_labels_test = encoder.transform(labels_test)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train_bow = joblib.load('processed_train_bow.save')\n",
            "X_train_tfidf = joblib.load('processed_train_tfidf.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "y_test_bow = joblib.load('processed_test_bow.save')\n",
            "y_test_tfidf = joblib.load('processed_test_tfidf.save')"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Support Vector Machines (SVM)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.svm import LinearSVC"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf_bow = LinearSVC(C=0.001)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf_bow.fit(X_text_bow, encoded_labels_train)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "preds_bow = clf_bow.predict(y_text_bow)\n",
            "print(classification_report(encoded_labels_test, preds_bow))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf_tfidf = LinearSVC(C=0.001)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clf_tfidf.fit(X_text_tfidf, encoded_labels_train)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "preds_tfidf = clf_tfidf.predict(y_text_tfidf)\n",
            "print(classification_report(encoded_labels_test, preds_tfidf))"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Multi layer perceptron"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import tensorflow as tf\n",
            "from tensorflow.keras import layers\n",
            "import scipy"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "TF-IDF"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "inputs = tf.keras.Input(shape=(25000))\n",
            "\n",
            "x = layers.Dense(128, activation=\"relu\")(inputs)\n",
            "x = layers.Dense(64, activation=\"relu\")(x)\n",
            "x = layers.Dense(32, activation=\"relu\")(x)\n",
            "prediction = layers.Dense(1, activation=\"sigmoid\")(x)\n",
            "\n",
            "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
            "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
            "    filepath='./checkpoint/best_model.h5',\n",
            "    save_weights_only=True,\n",
            "    monitor='val_accuracy',\n",
            "    mode='max',\n",
            "    save_best_only=True)\n",
            "model = tf.keras.Model(inputs, prediction)\n",
            "model.compile(loss=\"binary_crossentropy\",\n",
            "              optimizer=\"adam\", metrics=[\"accuracy\"], )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "X_train, X_val, Y_train, Y_val = train_test_split(\n",
            "    X_text_tfidf, encoded_labels_train, test_size=0.2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "history = model.fit(X_train, Y_train, epochs=100,\n",
            "          callbacks=[model_checkpoint_callback, callback], validation_data=(X_val, Y_val))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(history, './mlp_train_history.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model.save('./mlp_tfidf_save')\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "preds = model.predict(y_text_tfidf)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "preds = np.round(preds)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "predictions = []\n",
            "\n",
            "for item in preds:\n",
            "  predictions.append(int(item[0]))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "print(classification_report(encoded_labels_test, preds))"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "BoW"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "inputs = tf.keras.Input(shape=(25000))\n",
            "\n",
            "x = layers.Dense(128, activation=\"relu\")(inputs)\n",
            "x = layers.Dense(64, activation=\"relu\")(x)\n",
            "x = layers.Dense(32, activation=\"relu\")(x)\n",
            "prediction = layers.Dense(1, activation=\"sigmoid\")(x)\n",
            "\n",
            "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
            "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
            "    filepath='./checkpoint/best_model_bow.h5',\n",
            "    save_weights_only=True,\n",
            "    monitor='val_accuracy',\n",
            "    mode='max',\n",
            "    save_best_only=True)\n",
            "model = tf.keras.Model(inputs, prediction)\n",
            "model.compile(loss=\"binary_crossentropy\",\n",
            "              optimizer=\"adam\", metrics=[\"accuracy\"], )\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.model_selection import train_test_split\n",
            "\n",
            "X_train, X_val, Y_train, Y_val = train_test_split(\n",
            "    X_text_bow, encoded_labels_train, test_size=0.2)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "history = model.fit(X_train, Y_train, epochs=100,\n",
            "                    callbacks=[model_checkpoint_callback, callback], validation_data=(X_val, Y_val))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(history, './mlp_train_history.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model.save('./mlp_bow_save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "model = tf.keras.models.load_model('./mlp_bow_save')\n",
            "preds = model.predict(y_text_bow)\n",
            "preds = np.round(preds)\n",
            "predictions = []\n",
            "\n",
            "for item in preds:\n",
            "  predictions.append(int(item[0]))\n",
            "\n",
            "print(classification_report(encoded_labels_test, preds))"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Logistic Regression"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.linear_model import LogisticRegression,SGDClassifier"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# training the model\n",
            "lr = LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
            "\n",
            "# fitting the model for Bag of words\n",
            "lr_bow = lr.fit(X_text_bow, encoded_labels_train)\n",
            "print(lr_bow)\n",
            "\n",
            "# fitting the model for tfidf features\n",
            "lr_tfidf = lr.fit(X_text_tfidf, encoded_labels_train)\n",
            "print(lr_tfidf)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# predicting the model for bag of words\n",
            "lr_bow_predict = lr.predict(y_text_bow)\n",
            "print(lr_bow_predict)\n",
            "\n",
            "# predicting the model for tfidf features\n",
            "lr_tfidf_predict = lr.predict(y_text_tfidf)\n",
            "print(lr_tfidf_predict)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# accuracy score for bag of words\n",
            "lr_bow_score = accuracy_score(encoded_labels_test, lr_bow_predict)\n",
            "print(\"lr_bow_score :\",lr_bow_score)\n",
            "\n",
            "# accuracy score for tfidf features\n",
            "lr_tfidf_score = accuracy_score(encoded_labels_test, lr_tfidf_predict)\n",
            "print(\"lr_tfidf_score :\",lr_tfidf_score)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# classification report for bag of words \n",
            "lr_bow_report = classification_report(encoded_labels_test, lr_bow_predict, target_names=['Positive','Negative'])\n",
            "print(lr_bow_report)\n",
            "\n",
            "# classification report for tfidf features\n",
            "lr_tfidf_report = classification_report(encoded_labels_test, lr_tfidf_predict, target_names=['Positive','Negative'])\n",
            "print(lr_tfidf_report)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Text Clustering"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### Density Clustering"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.cluster import DBSCAN"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_pp = pd.read_csv('../data/preprocessed_train.csv')\n",
            "train_pp = train_pp[['text', 'sentiment', 'preprocessed_text']]\n",
            "\n",
            "test_pp = pd.read_csv('../data/preprocessed_test.csv')\n",
            "test_pp = test_pp[['text', 'sentiment', 'preprocessed_text']]\n",
            "\n",
            "X_train = train_pp['preprocessed_text']\n",
            "X_test = test_pp['preprocessed_text']"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "train_pp = pd.read_csv('../data/preprocessed_train.csv')\n",
            "test_pp = pd.read_csv('../data/preprocessed_test.csv')\n",
            "train_pp = train_pp['preprocessed_text']\n",
            "test_pp = test_pp['preprocessed_text']\n",
            "\n",
            "full_dataframe = pd.concat((train_pp, test_pp), axis=0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "full_dataframe.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.feature_extraction.text import TfidfVectorizer  # -- Tf-Idf\n",
            "\n",
            "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2),)\n",
            "\n",
            "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
            "X_test_tfidf = vectorizer.transform(X_test)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.decomposition import TruncatedSVD\n",
            "\n",
            "svd = TruncatedSVD(n_components=200, n_iter=100)\n",
            "data1 = svd.fit_transform(X_train_tfidf)\n",
            "data2 = svd.fit_transform(X_test_tfidf)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data1.shape"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(data1,'./svd_train.save')\n",
            "joblib.dump(data2,'./svd_test.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "data1 = joblib.load(\"./svd_test.save\")\n",
            "data2 = joblib.load(\"./svd_train.save\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "full_dataset_tfidf = np.concatenate((data1, data2))\n",
            "full_dataset_tfidf.shape"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "DBSCAN EPS 0.75"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.metrics import silhouette_score\n",
            "\n",
            "def get_silhouette(labels, data):\n",
            "  silhouette_avg = silhouette_score(data, labels )\n",
            "  print(f\"Silhouette is equal to {silhouette_avg}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm import tqdm\n",
            "clustering = DBSCAN(min_samples=3, eps=.25)\n",
            "clustering = clustering.fit_predict(data1)\n",
            "\n",
            "filtered_data = []\n",
            "filtered_labels = []\n",
            "for index in tqdm(range(0, len(clustering))):\n",
            "  label = clustering[index]\n",
            "  if label != -1:\n",
            "    filtered_data.append(data1[index])\n",
            "    filtered_labels.append(label)\n",
            "\n",
            "get_silhouette(filtered_labels, filtered_data)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
            "import matplotlib.pyplot as plt\n",
            "%matplotlib inline"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "clustering = joblib.load('./agglomerative_clustering.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def plot_word_cloud_data(data, clusters):\n",
            "  data_str = [str(x) for x in data]\n",
            "  datas = {\n",
            "      'text': data_str,\n",
            "    'cluster': clusters\n",
            "  }\n",
            "  new_data = pd.DataFrame(datas)\n",
            "  for cluster_num in np.unique(clusters):\n",
            "    if cluster_num != -1:\n",
            "\n",
            "      data_to_plot = new_data.loc[new_data['cluster'] == cluster_num]\n",
            "      print(type(data_to_plot['text'].values.tolist()[0]))\n",
            "      fullstring = ' '.join(data_to_plot['text'].values)\n",
            "      wordcloud = WordCloud(max_font_size=50, max_words=100,\n",
            "                            background_color=\"white\").generate(fullstring)\n",
            "      plt.figure()\n",
            "      plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
            "      plt.axis(\"off\")\n",
            "      plt.show()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot_word_cloud_data(full_dataframe.values, clustering)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "get_silhouette(filtered_labels, filtered_data)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.decomposition import PCA\n",
            "import seaborn as sns\n",
            "\n",
            "def plot_clustering(points, clusters):\n",
            "  pca = PCA(2)\n",
            "  data = pca.fit_transform(points)\n",
            "  print(data.shape)\n",
            "  df = pd.DataFrame(data, columns=['x','y'])\n",
            "  df['cluster'] = clusters\n",
            "  sns.scatterplot(data=df, x=\"x\", y=\"y\", hue=\"cluster\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot_clustering(filtered_data, filtered_labels)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "k-means"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.cluster import KMeans"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "kMeans = KMeans(n_clusters=4)\n",
            "clusters = kMeans.fit_predict(full_dataset_tfidf)\n",
            "get_silhouette(clusters, full_dataset_tfidf)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "joblib.dump(clusters, './clustering_kmeans.save')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot_word_cloud_data(full_dataframe, clusters)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "plot_clustering(full_dataset_tfidf, clusters)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "text",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.9"
      },
      "orig_nbformat": 4,
      "vscode": {
         "interpreter": {
            "hash": "69059b0c3dc4947ea569f494d4ae19bc6a000e056abcbb0d417b4d8be3f88c1b"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
